---
title: Zeek集群架构
toc: true
date: 2020-03-12 23:19:30
tags: [IDS, Zeek]
index_img: /img/zeek-logo.png
---

**为什么Zeek需要集群架构？因为Zeek是是单线程的，因此一旦达到单个处理器内核的限制，当前唯一的方法就是把工作负载分散到多个内核甚至是多个物理机上。**

<!--more-->

## 1 Cluster Architecture

![](https://raw.githubusercontent.com/QGrain/picgo-bed/master/figure/deployment.png)

上图是Zeek的集群架构示意图

- **Tap**是一种机制用来拆分数据流并创建副本用于检测。例如交换机上的监测端口和光纤网络上的分光路器。
- **Frontend**是一种离散的硬件设备或主机技术，可将流量分为许多流。Zeek的二进制文件并不执行此工作。
- **Manager**是一个Zeek的进程，具有两个主要任务。第一个是从集群的其他采用Zeek通信协议的节点接收日志和通知（如果你用的是logger则会取代manager来接收所有的日志）它的输出结果是一个单独的日志而不是许多需要被后期处理组合的离散的日志。第二个任务是它能够作为通知的阻塞节点来进行重复数据删除操作和通知行为化操作（如将通知以邮件形式发送，分页或者阻止）。
- **Logger**是一个可选的Zeek进程，它使用Zeek通信协议从群集的其余节点接收日志消息。让logger代替manager接收日志的目的是为了减轻manager的负担。
- **Proxy**是一个用于同步状态的Zeek进程，变量能够自动地在连接的Zeek进程间同步。proxy避免了所有worker都相互直连。
- **Worker**是嗅探网络流量并对重新组合的流量进行协议分析的Zeek进程。由于一个集群的所有协议解析和大多数分析都将在此处进行，因此建议采用尽可能高性能的内存和CPU。因为几乎所有日志记录都是在远程对manager完成的，并且通常很少写入磁盘，因此workers对磁盘没有特殊要求。

## 2 Frontend Options

### 2.1 分立的硬件流量平衡器

- **cPacket：**如果要监视一个或多个10G物理接口，建议的解决方案是使用cPacket的cFlow或cVu设备，这些设备将通过重写目标以太网MAC地址以使与特定流关联的每个数据包具有相同的目标MAC，来执行第2层负载平衡。然后可以将数据包直接传递到监视主机，在监视主机上，每个worker都有一个BPF Filter以将其可见性限制为仅该流，或者继续传递到商用交换机，以将流量拆分为多个以适配worker的1G接口。从而大大降低成本。
- **OpenFlow Switches：**基于OpenFlow的交换机直接在交换机上进行基于流的负载平衡，从而大大降低了许多用户的前端成本。**官方正在探索中...**

### 2.2 主机流量平衡机制

- **PF_RING： **Linux的PF_RING软件具有“群集”功能，该功能将在嗅探同一接口的多个进程之间进行基于流的负载平衡。这使您可以轻松地在单个物理主机中利用多个内核，因为Bro的主事件循环是单线程的，因此无法本机利用所有内核。如果要使用PF_RING，请参阅有关如何使用PF_RING配置Zeek的[文档](http://bro.org/documentation/load-balancing.html)。
- **Netmap：** FreeBSD有一个正在进行的名为Netmap的项目，该项目还将启用基于流的负载平衡。**官方正在探索中...**
- **Software Router：** 可以通过简单的配置用于基于流的负载平衡。由于Zeek的PF_RING支持，因此不建议在Linux上使用此解决方案，并且仅将其作为其他操作系统上的不得已的方法，因为该解决方案会因每个数据包在内核和用户域之间来回上下文切换数次而导致大量开销。